# Biase Variance tradeoff
## Introduction
Prior notions: underfitting, overfitting, generalization. Goal to find lowest possible generalization error. 
Training ML model to reduce objective function: $\sum_{i=1}^m\lVert f(x^{(i)train}) - y^{(i)train}\rVert_2^2$
Achieve lower error on test set (holdout)  $\sum_{i=1}^m\lVert f(x^{(i)test}) - y^{(i)test}\rVert_2^2$
We presume that training distribution ~ test distribution. 
![alt text](../images/bias-variance-trade-off.jpg)
*Image 1: Adopted from [paper](https://link.springer.com/article/10.1007/s10115-019-01335-4)*
## Data Parameters
With data available $\mathcal{Z}=\{(x^{(i)},y^{(i)})\}_i$ we assume they are i.i.d. - identically independently distributed. We want to fit some model $f(x; \theta,\Theta)$ for parameters $\theta$ and hyperparameters $\Theta$. 

!! Common mistake are various kinds of knowledge leak where information from holdout set leaks to choice of hyperparameters or parameters. 

!! Choosing hyperparameters (hp) on the dependent only of training set leads to high capacity models. To reduce these errors common way is to use Cross-Validation (CV) and holdout set. With CV hp can be estimated and yield biased solutions. CV( [Nested CV, @cawley2010over](https://www.jmlr.org/papers/volume11/cawley10a/cawley10a.pdf), [BBC-CV, @tsamardinos2018bootstrapping](https://github.com/mensxmachina/BBC-CV)) vs Deep Learning Models they leverage [double descent, @belkin2019reconciling](https://arxiv.org/pdf/1812.11118). draw what double descent is.

Lets say our real data are generated by some $f(\theta)$, to evaluate we use point wise $f(\hat\theta)$ and choose hp that lead towards optimal solution. To evaluate the parameters of the real distribution generators we have measured data $\mathcal{Z}$. (Our measurements have errors and are prone to experimental error)

## Freqeuentist Bias and Variance
Looking at the parameters from statistics point of view they are $\hat\theta := g(\mathcal{Z})$ some statistics of the data. bias of this statistics is the difference defined as: $Bias[g(\mathcal{Z})] = \mathbb{E}[g(\mathcal{Z})] - \theta$ same for the Variance of these statistics
$Var[g(\mathcal{Z})] = \mathbb{E}[(g(\mathcal{Z}) - \theta)^2]$.

In example of $p \sim Bernoulli(\theta)$ that is $p\sim \{p(0)=1-\theta, p(1)=\theta\}$ with density function: 
$$p(x^{(i)};\theta) =\theta^{x^{(i)}}(1-\theta)^{x^{(i)}}.$$
 Define $g$ to be, say estimator of parameters $\hat\theta$: $g(\mathcal{Z}) = \hat\theta = \frac{1}{m}\sum_i x^{(i)}$.

Calculation of $Bias[g(\mathcal{Z})]$ and $Var[g(\mathcal{Z})]$:

$$Bias[g(\mathcal{Z})] = \mathbb{E}[g(\mathcal{Z})] - \theta =$$
$$= \mathbb{E}[\frac{1}{m}\sum_i x^{(i)}] - \theta = \frac{1}{m}\sum_i \mathbb{E}[x^{(i)}] - \theta =$$
$$= \frac{1}{m}\sum_i \sum_{x^{(i)}\in\{0,1\}} x^{(i)} \theta^{x^{(i)}}(1-\theta)^{x^{(i)}} - \theta =$$
$$= \frac{1}{m}\sum_i[0\theta^0(1-\theta)^0 + 1\theta^1(1-\theta)^0] -\theta = 0.$$
Variance the second moment of the statistics $g(\mathcal{Z})$ usually defined as $Var(g(\mathcal{Z})) :=\mathbb{E}[(g(\mathcal{Z}) - \theta)^2]$ We arrive at:
$$Var(g(\mathcal{Z})) = \mathbb{E}[g^2(\mathcal{Z}) - g(\mathcal{Z})\theta - \theta g(\mathcal{Z}) + \theta^2] = $$
$$= \mathbb{E}[g^2(\mathcal{Z})] - \mathbb{E}[g(\mathcal{Z})]\theta - \theta \mathbb{E}[g(\mathcal{Z})] + \mathbb{E}[\theta^2] =$$ 
if $g(\mathcal{Z}) = \frac{1}{m}\sum_i x^{(i)}$ and $X \sim Bernoully(\theta)$ then: 
$$= \mathbb{E}[\hat \theta^2] - \theta^2$$

## Bias Variance trade-off
Mean square error $MSE = \mathbb{E}[(\mathbb{E}[\hat\theta] - \theta)^2]$ but real parameters that produce the data can really never be measured are prone to basic noise $ $noise(\epsilon) = \mathbb{E}[\theta^2-\tilde\theta^2]$ where $\theta$ is something uncertain, and has some confounding factor that is essential part of it. We try to approximate $\tilde\theta$.

Calculation: ...

$MSE = \mathbb{E}[(\mathbb{E}[\hat\theta] - \theta)^2] = Bias^2(\hat\theta) + Var(\hat\theta) + noise(\epsilon)$

Bias variance trade-off is derived from $MSE$, but in Deep Learning we use cross entropy loss [cross entropy, @yang2020rethinking](https://proceedings.mlr.press/v119/yang20j/yang20j.pdf), or for ranking algorithm as in recomendatio [ranking, @shivaswamy2021bias](https://dl.acm.org/doi/10.1145/3437963.3441772)



![alt text](../images/foundations-bias-variance-00.svg)