# Maximum Likelihood Estimation (MLE) - Procjenitelj maksimalne izglednosti
The idea is to find the parameter values that maximize the likelihood of the observed data under the model's assumptions.

We start with a parametric model $p_{model}(\mathbf{x}; \mathbf{\theta})$, which represents the probability of data $\mathbf{x}$ given parameters $\mathbf{\theta}$. Our goal is to estimate the value of $\mathbf{\theta}$ that makes the observed data most probable.

Given a dataset $\mathcal{X} = \{\mathbf{x}^{(i)}\}_{i=1}^m$, the likelihood function $L(\mathbf{\theta})$ is defined as:

$$
L(\mathbf{\theta}) = \prod_{i=1}^m p_{model}(\mathbf{x}^{(i)}; \mathbf{\theta})
$$

Alternatively, it's common to work with the log-likelihood function, which is easier to optimize:

$$
\log L(\mathbf{\theta}) = \sum_{i=1}^m \log p_{model}(\mathbf{x}^{(i)}; \mathbf{\theta})
$$

### Optimization 
The maximum likelihood estimate $\hat{\mathbf{\theta}}$ is the value of $\mathbf{\theta}$ parameters that maximizes the log-likelihood.

$$
\hat{\mathbf{\theta}} = \arg\max_{\mathbf{\theta}} \log L(\mathbf{\theta})
$$
Monotone functions do not influence the optimization task same theta will provide the maximum of the objective. These kind of functions are commute with optimization and finding the best parameters, or the MLE of $\hat\theta$.

$$
\hat{\mathbf{\theta}} = \arg\max_{\mathbf{\theta}} \frac{1}{m}\sum_{i=1}^m \log p_{model}(\mathbf{x}^{(i)}; \mathbf{\theta}) = \arg\max_{\mathbf{\theta}} \mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \left[ \log p_{model}(\mathbf{x}^{(i)}; \mathbf{\theta})\right]
$$

- Over complex model $p_{model}(\mathbf{x}; \mathbf{\theta})$ or ignoring prior information can lead to overfitting or incorrect estimates. Prior information can be added through Maximum a posteriori probability (MAP) estimation.
- A common mistake is assuming that the data follows a specific distribution without verifying assumptions. 
- MLE is consistent $\hat{\mathbf{\theta}} \to \mathbf{\theta}$ as $m \to \infty$.
- MLE is inherently related to the bias-variance tradeoff. Unregularized MLE may lead to low bias but high variance, especially in high-dimensional models, resulting in overfitting. Complexity $\implies$ possibility of overfitting $\implies$ high variance.
- To mitigate overfitting, techniques like regularization $L_2$ and $L_1$ norms or Bayesian methods can be employed, introducing bias but reducing variance, leading to better generalization.

### KL Divergence in Model Training
The Kullback-Leibler (KL) divergence is a measure of how one probability distribution differs from another. One way to interpret MLE is to view it as minimizing the dissimilarity between the **empirical distribution** $\hat{p}_{data}(\mathbf{x})$ with the model's **predicted distribution** $p_{model}(\mathbf{x};\mathbf{\theta})$. The goal is to train the model such that $p_{model}(\mathbf{x};\mathbf{\theta})$ becomes as close as possible to $\hat{p}_{data}(\mathbf{x})$.

$$
D_\text{KL}(\hat p_{data}(\mathbf{x}) \parallel p_{model}(\mathbf{x};\mathbf{\theta})) = \mathbb{E}_{\mathbf{x} \sim \hat p_{data}} \left[ \log \hat p_{data}(\mathbf{x}) - \log p_{model}(\mathbf{x};\mathbf{\theta}) \right]
$$
where the optimisation problem would be represented as finding the optimal $\hat{\mathbf{\theta}}$ by computing the $\arg \min_{\mathbf{\theta}} D_\text{KL}(\hat p_{data}(x) \parallel p_{model}(\mathbf{x};\mathbf{\theta}))$. 
In this expression:
- $\mathbb{E}_{\mathbf{x} \sim \hat p_{data}}$ is the expectation over the data distribution $\hat{p}_{data}(\mathbf{x})$.
- $\hat{p}_{data}(\mathbf{x})$ is the empirically measured distribution of the data we are working with.
- $\log \hat{p}_{data}(\mathbf{x})$ represents the log-probability under the true data distribution.
- $\log p_{model}(\mathbf{x})$ represents the log-probability of the model's distribution.

Since the term $\log \hat{p}_{data}(\mathbf{x})$ does not depend on the model parameters is to minimize the KL divergence is equivalent to **minimizing** the negative log-likelihood (NLL) of the model:

$$
- \mathbb{E}_{\mathbf{\mathbf{x}} \sim \hat p_{data}} \left[ \log p_{model}(\mathbf{x};\mathbf{\theta}) \right]
$$

This is same as maximizing the likelihood of the data under the model's distribution $p_{model}(\mathbf{x};\mathbf{\theta})$. Minimizing KL divergence between the data distribution and the model distribution is fundamentally the same as performing MLE for $\mathbf{X}$ random variable. 
 
### Cross-Entropy

Cross-entropy measures the difference between two probability distributions: the true data distribution $\hat{p}_{data}(\mathbf{x})$ and the model distribution $p_{model}(\mathbf{x};\mathbf{\theta})$. It is defined as:

$$
H(\hat{p}_{data}, p_{model}) = -\mathbb{E}_{\mathbf{x} \sim \hat{p}_{data}} \left[ \log p_{model}(\mathbf{x};\mathbf{\theta}) \right]
$$

This measures how well the model's predicted distribution $p_{model}(\mathbf{x})$ aligns with the true data measured distribution $\hat{p}_{data}(\mathbf{x})$. The smaller the cross-entropy, the better the model's predictions match the true data.

> We can thus see maximum likelihood as an attempt to make the model distribution match the empirical distribution $\hat{p}_{data}$. Ideally, we would like to match the true data-generating distribution $p_{data}$, but we have no direct access to this distribution.

> *Any loss consisting of a negative log-likelihood is a cross-entropy between the empirical distribution deﬁned by the training set and the probability distribution deﬁned by model. In terms of parameter optimization.* 

### Conditional MLE
Up until now we were talking about the MLE of a random variable of some observed data. What about MLE of a classifier or a regressor?

Conditional Maximum Likelihood Estimation (MLE) is a variation of MLE where we estimate the parameters of a model conditional on the inputs $\mathbf{x}$. In this case, the goal is to find the parameters $\mathbf{\theta}$ that maximize the likelihood of the target variable $y$ given the inputs $\mathbf{x}$.

For a dataset $\mathcal{X} = \{(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})\}_{i=1}^m$, the conditional likelihood function for each data point is $p_{model}(\mathbf{y}^{(i)}|\mathbf{x}^{(i)}; \mathbf{\theta})$, and the overall conditional likelihood is:

$$
L(\mathbf{\theta}) = \prod_{i=1}^m p_{model}(y^{(i)}|\mathbf{x}^{(i)}; \mathbf{\theta})
$$

The log-likelihood is:

$$
\log L(\mathbf{\theta}) = \sum_{i=1}^m \log p_{model}(y^{(i)}|\mathbf{x}^{(i)}; \mathbf{\theta})
$$

The parameters $\mathbf{\theta}$ that maximize this log-likelihood are the conditional maximum likelihood estimates:

$$
\hat{\mathbf{\theta}} = \arg\max_{\mathbf{\theta}} \log L(\mathbf{\theta}) = \arg\max_{\mathbf{\theta}} \mathbb{E}_{(y, \mathbf{x}) \sim \hat p_{data}}[\log p_{model}(y^{(i)}|\mathbf{x}^{(i)}; \mathbf{\theta})]
$$

### Linear regression as Maximum Likelihood
Previous motivation of liner regression as an algorithm that learns to take an input $\mathbf{x}$ and produce output $\hat y$. The mapping from $\mathbf{x}$ to $y$ is chosen to minimize mean squared error (MSE), criterion that we introduced more or less arbitrarily. 

Instead of producing a single prediction $\hat y$, we now think of the model as producing a conditional distribution $p(y|\mathbf{x})$. 

>We can imagine that with an infinitely large training set same $\mathbf{x}$ could map to different $y$ values.

In this example, we assume that the variance is fixed to some constant $\sigma^2$ chosen by the user. The model function $f(\cdot;\mathbf{\theta}): \mathcal{X} \rightarrow \mathcal{Y}$ gives the prediction of the mean of the Gaussian for each point $\mathbf{x}$ so that $f(\mathbf{x};\mathbf{\theta}) = \hat{y}$.  

We will see that this choice $p_{model}(y | \mathbf{x})$ causes the maximum likelihood estimation procedure to yield the same learning algorithm as we developed before optimizing $MSE = \frac{1}{m} \sum_{i=1}^m \left( \hat{y}^{(i)} - y^{(i)} \right)^2$.

$$
p(y | \mathbf{x}) \sim \mathcal{N}(\hat{y}, \sigma^2).
$$

The probability density function (PDF) of normal distribution is:  

$$
p(y | \mathbf{x}) = \frac{1}{\sqrt{2\pi \sigma^2}} \exp\left(-\frac{(y - \hat{y}(\mathbf{x}; \mathbf{w}))^2}{2\sigma^2}\right). \implies
$$
that the NLL of the model and parameters is:
$$
NLL(\mathbf{\theta}) = \arg \min_\mathbf{\theta} - \sum_{i=1}^m \log p(y^{(i)} | \mathbf{x}^{(i)}; \theta) = 
$$
$$= - \left(-m \log \sigma - \frac{m}{2} \log(2\pi) + \sum_{i=1}^m \frac{\left( \hat{y}^{(i)} - y^{(i)} \right)^2}{2\sigma^2} \right) \sim
$$

$$
\sim \arg \min_{\mathbf{\theta}}\sum_{i=1}^m \left( \hat{y}^{(i)} - y^{(i)} \right)^2
$$
### Summary
- Conditional MLE is used to estimate parameters of a model based on inputs $\mathbf{x}$.
- Linear regression can be viewed as a special case of conditional MLE when the target is conditionally Gaussian.
- Minimizing the Mean Squared Error (MSE) is equivalent to maximizing the conditional log-likelihood in linear regression. Optimization of both minimizing MSE and NLL is equivalent. 

### Supplements

"Statistical efficiency is typically studied in the parametric case(as in regression), where our goal is to estimate the value of a parameter (assuming it is possible to identify the true parameter), not the value of a function. A way to measure how close we are to the true parameter is by the expected mean squared error, computing the squared difference between the estimated and true parameter values, where the expectation is over $m$ training samples from the data-generating distribution. That parametric mean squared error decreases as $m$ increases, and formlarge, the Cramér-Rao lower bound" \cite{goodfellow2016deep}.
The Cramér-Rao Lower Bound provides a lower limit for the variance of any unbiased estimator of a parameter, with the variance being inversely related to the Fisher Information. It is a key result in parameter estimation, helping to define how well we can estimate a parameter based on available data. Fisher Information essentially tells us how much sharpness (information) there is in the likelihood function with respect to $\theta$. A higher Fisher Information means the parameter can be estimated more precisely. 


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    displayMath: [['$$', '$$'], ['\\[', '\\]']]
  },
  messageStyle: "none"
});

</script>